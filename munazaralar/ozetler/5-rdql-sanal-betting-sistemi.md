# RDQL Sanal Betting Sistemi - Ã–ÄŸrenen & GeliÅŸen AI

## ğŸ¯ Proje Vizyonu
**"API ile beslenen, sÃ¼rekli Ã¶ÄŸrenen, kupon stratejileri geliÅŸtiren, sanal dÃ¼nyada kazanÃ§ elde eden AI sistemi"**

Pre-match ve live-match verilerini API'den Ã§ekerek; takÄ±m, lig, oyuncu, formasyon ve oyun gÃ¼cÃ¼ konularÄ±nda Ã¶ÄŸrenen ve geliÅŸen; kupon stratejileri geliÅŸtirerek sanal ortamda (paper trading) kazanÃ§ elde eden bir Recurrent Deep Q-Learning (RDQL) sistemi.

## ğŸ—ï¸ 1. Veri Mimarisi (3-Tier Storage)

### Kritik Tespit: CanlÄ± Veri YÃ¶netimi Sorunu
**Sorun:** CanlÄ± maÃ§larda bazÄ± veriler kÃ¼mÃ¼latif (korner), bazÄ±larÄ± dalgalÄ± (hakimiyet), oranlar sÃ¼rekli deÄŸiÅŸir, bahis tÃ¼rleri aÃ§Ä±lÄ±r/kapanÄ±r.

**Ã‡Ã¶zÃ¼m:** Hot-Warm-Cold 3-Tier Mimari

### Teknoloji KararlarÄ± (5 Tur MÃ¼nazara Sonucu)

**Ana Depolama: ClickHouse** (TimescaleDB yerine)
- âœ… 1M/s tick ingestion (TimescaleDB'den 5x hÄ±zlÄ±)
- âœ… Vectorized sorgular (PB scale historical data)
- âœ… ReplacingMergeTree (idempotent odds upsert)
- âœ… Materialized Views (gerÃ§ek zamanlÄ± hakimiyet rollup)
- âœ… TTL ile S3'e soÄŸuk taÅŸÄ±ma

```sql
-- ClickHouse Tablo YapÄ±sÄ±
CREATE TABLE live_odds (
    time DateTime64(3),
    match_id UInt32,
    market LowCardinality(String),
    odds Float64,
    version UInt64
) ENGINE = ReplacingMergeTree(version)
ORDER BY (time, match_id, market)
PARTITION BY toYYYYMMDD(time)
TTL time + INTERVAL 30 DAY TO DISK 's3_cold';

-- Materialized View (Hakimiyet Rollup)
CREATE MATERIALIZED VIEW dominance_1s_mv
ENGINE = AggregatingMergeTree()
ORDER BY (match_id, time_bucket)
AS SELECT
    match_id,
    toStartOfSecond(time) AS time_bucket,
    avgState(dominance) AS avg_dominance,
    maxState(dominance) AS max_dominance
FROM live_stats
GROUP BY match_id, time_bucket;
```

**Feature Store: Feast** (Online + Offline)
- **Online Store:** Redis (sub-millisecond latency)
- **Offline Store:** Delta Lake (Hudi MOR format)

### CDC Pipeline: ClickHouse â†’ Feast

**Sorun:** ClickHouse native CDC desteklemez.

**Ã‡Ã¶zÃ¼m:** Kafka Engine + Materialized View

```sql
-- Kafka Engine Tablosu
CREATE TABLE feast_cdc (
    match_id String,
    feature_name String,
    value Float64,
    event_ts DateTime64(3)
) ENGINE = Kafka
SETTINGS kafka_broker_list = 'kafka:9092',
         kafka_topic_list = 'feast_features',
         kafka_format = 'JSONEachRow';

-- Materialized View (Latest State Push)
CREATE MATERIALIZED VIEW feast_cdc_mv TO feast_cdc AS
SELECT
    match_id,
    feature_name,
    argMax(value, event_ts) AS value,  -- Latest value
    max(event_ts) AS event_ts
FROM live_features FINAL
GROUP BY match_id, feature_name;
```

**Flink CDC Pipeline:**
```
ClickHouse Kafka Engine â†’ Flink (exactly-once) â†’ Feast
                                â†“
                    Redis (Lua CAS version check)
                    Delta Lake (Hudi MOR upsert)
```

### Hudi Merge-on-Read (MOR) Stratejisi

**Ã‡atÄ±ÅŸma:** Copy-on-Write vs Merge-on-Read

**Karar:** Merge-on-Read (MOR) - YÃ¼ksek frekanslÄ± gÃ¼ncellemeler iÃ§in

**GerekÃ§e:**
- âœ… Log-append yazma (write amplification %60 azaltma)
- âœ… Async compaction (5-10 dakika)
- âœ… 128MB log file size
- âœ… Hourly clustering

```yaml
# Hudi Table Configuration
hudi_table:
  type: MERGE_ON_READ
  recordkey.field: "match_id,feature"
  precombine.field: "event_ts"
  partition.path.field: "match_date"
  hoodie.compaction.async.enabled: true
  hoodie.compaction.async.wait.timeout: 300
  hoodie.compaction.strategy: BINLOG
  hoodie.compaction.max.delta: 10
  hoodie.log.file.size: 128MB
  hoodie.small.file.limit: 100MB
```

## ğŸ§  2. Hibrit Model Mimarisi (Graph-LSTM + TFT)

### Ã‡atÄ±ÅŸma Ã‡Ã¶zÃ¼mÃ¼: Kalman vs LSTM-State-Space
**Karar:** LSTM-State-Space (Kalman'Ä±n lineer varsayÄ±mÄ± futbola uymaz)

### 3-KatmanlÄ± Mimari

**1. Graph-LSTM Encoder (Spatial-Temporal)**
- **AmaÃ§:** Oyuncu/takÄ±m iliÅŸkilerini ve zaman serisini birleÅŸtirme
- **Teknoloji:** GNN (Graph Neural Networks) + LSTM

```python
class GraphLSTM(nn.Module):
    def __init__(self, node_dim=64, hidden_dim=128):
        super().__init__()
        self.gnn = GraphConv(node_dim, hidden_dim)
        self.lstm = nn.LSTM(hidden_dim, hidden_dim)
        
    def forward(self, x_nodes, edge_index, batch_index):
        # GNN: Spatial embedding (oyuncu etkileÅŸimleri)
        x_graph = self.gnn(x_nodes, edge_index)
        
        # Global Attention Pooling (node â†’ graph vector)
        pooled = global_attention_pool(x_graph, batch_index)
        
        # Sequence formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r (Batch, Seq, Embed)
        lstm_input = pooled.view(batch_size, seq_len, -1)
        
        # LSTM: Temporal dynamics
        output, (h, c) = self.lstm(lstm_input)
        
        return output, h, c
```

**2. LSTM-State-Space Core (Dynamics Modeling)**
- **AmaÃ§:** Oyun dinamiklerini non-lineer modelleme
- **FormÃ¼l:** $h_t = \sigma(W_h [h_{t-1}, x_t] + b_h)$

```python
class LSTMStateSpace(nn.Module):
    def forward(self, spatial_embedding, market_data):
        # Spatial + Market verilerini birleÅŸtir
        x_t = torch.cat([spatial_embedding, market_data], dim=-1)
        
        # LSTM dynamics
        h_t, c_t = self.lstm_cell(x_t, (h_prev, c_prev))
        
        # Residual connection (daha iyi gradient flow)
        h_t = h_t + spatial_embedding
        
        return h_t, c_t
```

**3. TFT Decoder (Variable Selection + Interpretability)**
- **AmaÃ§:** Hangi Ã¶zelliÄŸin (korner, oran, formasyon) Ã¶nemli olduÄŸunu belirleme
- **Teknoloji:** Temporal Fusion Transformer

```python
class TemporalFusionTransformer(nn.Module):
    def forward(self, lstm_state, static_features):
        # Variable Selection Network
        # Hangi feature'Ä±n o an Ã¶nemli olduÄŸunu Ã¶ÄŸren
        attention_weights = self.variable_selection(
            lstm_state, 
            static_features  # lig, takÄ±m gÃ¼cÃ¼, formasyon
        )
        
        # Weighted feature combination
        output = torch.sum(
            attention_weights.unsqueeze(-1) * lstm_state,
            dim=1
        )
        
        return output, attention_weights
```

### Entegre Model

```python
class IntegratedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.graph_lstm = GraphLSTM()      # Encoder
        self.lstm_core = LSTMStateSpace()  # Core
        self.tft_decoder = TemporalFusionTransformer()  # Decoder
    
    def forward(self, x_player, edge_index, market_data, static_features):
        # 1. Graph-LSTM Encoder (Spatial-Temporal)
        spatial_embedding, h, c = self.graph_lstm(x_player, edge_index)
        
        # 2. LSTM-State-Space Core (Dynamics)
        temporal_state, _ = self.lstm_core(spatial_embedding, market_data)
        
        # 3. TFT Decoder (Variable Selection)
        output, attention_weights = self.tft_decoder(
            temporal_state, 
            static_features
        )
        
        return output, attention_weights  # RL agent'a hem tahmin hem interpretability
```

### Graph-LSTM Veri FormatÄ±

**Sorun:** GNN Ã§Ä±ktÄ±sÄ± LSTM'e nasÄ±l beslenecek?

**Ã‡Ã¶zÃ¼m:** Global Attention Pooling + Sequence Format

```python
# Her zaman adÄ±mÄ± iÃ§in:
# 1. GNN node embeddings â†’ Global Attention Pooling â†’ graph vector
g_t = global_attention_pool(node_embeddings, batch_index)

# 2. Market verileriyle birleÅŸtir
x_t = torch.cat([g_t, market_odds, dominance], dim=-1)

# 3. Sequence formatÄ±nda LSTM'e besle
lstm_input = x_t.view(batch_size, seq_len, embed_dim)  # (B, T, D)
```

## ğŸ¤– 3. RDQL Ã–ÄŸrenme Stratejisi

### Recurrent Deep Q-Learning (RDQL)
**AmaÃ§:** Kupon stratejileri geliÅŸtirerek sanal kazanÃ§ elde etme

### CVaR-KÄ±sÄ±tlÄ± Reward Fonksiyonu

```python
class RDQLAgent:
    def __init__(self):
        self.model = IntegratedModel()
        self.cvar_limit = 0.05  # %5 risk limiti
        
    def compute_reward(self, action, outcome):
        # ROI hesapla
        roi = (outcome.payout - action.stake) / action.stake
        
        # CVaR kÄ±sÄ±tÄ± (Risk yÃ¶netimi)
        portfolio_risk = self.compute_cvar(self.portfolio)
        
        if portfolio_risk < self.cvar_limit:
            penalty = -10.0  # Risk limiti ihlali
        else:
            penalty = 0.0
        
        # Reward = ROI + Risk Penalty + Latency Penalty
        reward = roi + penalty - 0.01 * action.latency
        
        return reward
```

### Kelly Criterion + CVaR Optimization

```python
def kelly_stake(prob, odds, cvar_limit=0.05):
    # Kelly Criterion
    kelly_fraction = (prob * odds - 1) / (odds - 1)
    
    # CVaR kÄ±sÄ±tÄ± ile ayarla
    portfolio_var = compute_portfolio_variance()
    
    if portfolio_var > cvar_limit:
        # Risk Ã§ok yÃ¼ksek, stake'i azalt
        adjusted_fraction = kelly_fraction * 0.5
    else:
        adjusted_fraction = kelly_fraction
    
    return max(0, min(adjusted_fraction, 0.25))  # Max %25 bankroll
```

### DaÄŸÄ±tÄ±k EÄŸitim: Ray.io

```python
import ray
from ray import tune

@ray.remote
class RDQLWorker:
    def __init__(self, model_config):
        self.agent = RDQLAgent(model_config)
        
    def train_episode(self, match_data):
        # Sanal simÃ¼lasyon
        env = PettingZooEnv(match_data)
        
        state = env.reset()
        done = False
        
        while not done:
            # Model inference
            action = self.agent.select_action(state)
            
            # Environment step
            next_state, reward, done, info = env.step(action)
            
            # Experience replay
            self.agent.replay_buffer.add(
                state, action, reward, next_state, done
            )
            
            # Train
            if len(self.agent.replay_buffer) > 1000:
                self.agent.train_batch()
            
            state = next_state
        
        return self.agent.get_metrics()

# DaÄŸÄ±tÄ±k eÄŸitim
ray.init()
workers = [RDQLWorker.remote(config) for _ in range(8)]
results = ray.get([w.train_episode.remote(data) for w in workers])
```

## ğŸ“Š 4. Feast Feature Engineering

### GNN Verisi Saklama (Protobuf)

```python
# GNN graph'Ä±nÄ± Protobuf olarak sakla
import protobuf

def encode_graph(nodes, edges):
    graph_proto = GraphProto()
    graph_proto.nodes.extend(nodes.tolist())
    graph_proto.edges.extend(edges.tolist())
    return graph_proto.SerializeToString()

# Feast Feature View
from feast import Feature, FeatureView, Entity

match_entity = Entity(name="match_id", join_key="match_id")

graph_feature_view = FeatureView(
    name="graph_embeddings",
    entities=[match_entity],
    features=[
        Feature(name="graph_blob", dtype="bytes"),  # Protobuf
        Feature(name="dominance", dtype="float"),
        Feature(name="odds", dtype="float")
    ],
    batch_source="clickhouse_source",
    online_store="redis"
)
```

### Historical Features (EÄŸitim Seti)

```python
class SpatialTemporalDataset(Dataset):
    def __init__(self, feast_client, entity_df):
        # Feast'ten historical features Ã§ek
        self.features = feast_client.get_historical_features(
            entity_df=entity_df,
            features=[
                "graph_embeddings:graph_blob",
                "graph_embeddings:dominance",
                "graph_embeddings:odds"
            ]
        ).to_df()
        
    def __getitem__(self, idx):
        # Protobuf decode
        graph_blob = self.features.iloc[idx]['graph_blob']
        graph_tensor = decode_protobuf(graph_blob)
        
        # Global Mean Pooling
        pooled = torch.mean(graph_tensor, dim=0)
        
        # Market features
        market = torch.tensor([
            self.features.iloc[idx]['dominance'],
            self.features.iloc[idx]['odds']
        ])
        
        # Concatenate
        x_t = torch.cat([pooled, market])
        
        return x_t, self.features.iloc[idx]['outcome']
```

## ğŸ® 5. Sanal SimÃ¼lasyon OrtamÄ±

### PettingZoo Multi-Agent Environment

```python
from pettingzoo import AECEnv

class BettingEnv(AECEnv):
    def __init__(self, match_data):
        self.match_data = match_data
        self.agents = ["agent_0"]  # Tek agent (sanal betting)
        self.bankroll = 1000.0
        
    def step(self, action):
        # Action: (bet_type, stake, odds)
        bet_type, stake, odds = action
        
        # SimÃ¼lasyon: MaÃ§ sonucunu tahmin et
        outcome = self.simulate_match()
        
        # Reward hesapla
        if outcome == bet_type:
            payout = stake * odds
            reward = payout - stake
        else:
            reward = -stake
        
        # Bankroll gÃ¼ncelle
        self.bankroll += reward
        
        # State gÃ¼ncelle
        next_state = self.get_state()
        
        done = self.bankroll <= 0 or self.match_ended
        
        return next_state, reward, done, {}
```

## ğŸ¯ 6. Kritik Teknik Kararlar

### 1. Veri Depolama
- âŒ TimescaleDB (WAL ÅŸiÅŸmesi, dÃ¼ÅŸÃ¼k ingestion)
- âœ… ClickHouse (1M/s ingestion, vectorized queries)

### 2. CDC Stratejisi
- âŒ Debezium (ClickHouse native CDC yok)
- âœ… Kafka Engine + Materialized View

### 3. Hudi Upsert
- âŒ Copy-on-Write (write amplification)
- âœ… Merge-on-Read (%60 yazma yÃ¼kÃ¼ azaltma)

### 4. Model SeÃ§imi
- âŒ Kalman Filter (lineer varsayÄ±m)
- âœ… LSTM-State-Space (non-lineer dynamics)

### 5. Pooling Stratejisi
- âœ… Global Attention Pooling (Mean + Attention hibrit)

### 6. Feature Store
- âœ… Feast (Redis online + Hudi MOR offline)
- âœ… GNN verisi Protobuf/bytes formatÄ±nda

## ğŸ“ 7. Eksiksiz Sistem Mimarisi

```
ğŸ“¦ RDQL SANAL BETTING SYSTEM
â”‚
â”œâ”€â”€ ğŸŒ DATA INGESTION
â”‚   â”œâ”€â”€ API Gateway (Pre-match + Live)
â”‚   â””â”€â”€ Kafka Topics (prematch, live, odds)
â”‚
â”œâ”€â”€ ğŸ’¾ STORAGE (3-Tier)
â”‚   â”œâ”€â”€ ClickHouse (Ana Depo)
â”‚   â”‚   â”œâ”€â”€ ReplacingMergeTree (odds upsert)
â”‚   â”‚   â”œâ”€â”€ Materialized Views (rollup)
â”‚   â”‚   â””â”€â”€ Kafka Engine (CDC)
â”‚   â”œâ”€â”€ Feast Feature Store
â”‚   â”‚   â”œâ”€â”€ Redis (Online - sub-ms)
â”‚   â”‚   â””â”€â”€ Delta Lake (Offline - Hudi MOR)
â”‚   â””â”€â”€ S3 (Cold Storage - TTL)
â”‚
â”œâ”€â”€ ğŸ”„ STREAM PROCESSING
â”‚   â””â”€â”€ Flink CDC
â”‚       â”œâ”€â”€ ClickHouse Kafka Engine â†’ Flink
â”‚       â”œâ”€â”€ Redis (Lua CAS version check)
â”‚       â””â”€â”€ Delta Lake (Hudi MOR upsert)
â”‚
â”œâ”€â”€ ğŸ§  AI/ML LAYER
â”‚   â”œâ”€â”€ Graph-LSTM Encoder
â”‚   â”‚   â”œâ”€â”€ GNN (Spatial)
â”‚   â”‚   â”œâ”€â”€ LSTM (Temporal)
â”‚   â”‚   â””â”€â”€ Global Attention Pooling
â”‚   â”œâ”€â”€ LSTM-State-Space Core
â”‚   â”‚   â””â”€â”€ Non-linear Dynamics
â”‚   â””â”€â”€ TFT Decoder
â”‚       â””â”€â”€ Variable Selection + Interpretability
â”‚
â”œâ”€â”€ ğŸ¤– RDQL AGENT
â”‚   â”œâ”€â”€ CVaR-KÄ±sÄ±tlÄ± Reward
â”‚   â”œâ”€â”€ Kelly Criterion Stake
â”‚   â””â”€â”€ Experience Replay
â”‚
â”œâ”€â”€ ğŸ® SIMULATION
â”‚   â”œâ”€â”€ PettingZoo Environment
â”‚   â””â”€â”€ Ray.io (DaÄŸÄ±tÄ±k EÄŸitim)
â”‚
â””â”€â”€ ğŸ“Š MONITORING
    â”œâ”€â”€ Prometheus Metrics
    â””â”€â”€ Grafana Dashboard
```

## ğŸš€ SonuÃ§

Bu sistem:
1. âœ… **SÃ¼rekli Ã¶ÄŸrenir** (RDQL + Experience Replay)
2. âœ… **GeliÅŸir** (Ray.io daÄŸÄ±tÄ±k eÄŸitim)
3. âœ… **Strateji geliÅŸtirir** (Kelly + CVaR)
4. âœ… **Sanal kazanÃ§ elde eder** (PettingZoo simÃ¼lasyon)
5. âœ… **CanlÄ± veriyi yÃ¶netir** (ClickHouse + Flink CDC)
6. âœ… **Yorumlanabilir** (TFT Variable Selection)

**Kritik BaÅŸarÄ± FaktÃ¶rleri:**
- ClickHouse (1M/s ingestion)
- Kafka Engine CDC (native CDC olmadan)
- Hudi MOR (%60 yazma optimizasyonu)
- Graph-LSTM + TFT (spatial-temporal + interpretability)
- LSTM-State-Space (non-lineer dynamics)
- Global Attention Pooling (node â†’ graph)
- CVaR-kÄ±sÄ±tlÄ± RDQL (risk yÃ¶netimi)

