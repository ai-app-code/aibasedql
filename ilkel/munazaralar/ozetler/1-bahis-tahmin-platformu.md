# Bahis Tahmin Platformu - AI TabanlÄ± Sistem TasarÄ±mÄ±


## ğŸ¯ Proje Hedefi
CanlÄ± maÃ§lar ve maÃ§ Ã¶ncesi bahis bÃ¼lteni verilerini kullanan, RL (Reinforcement Learning), QL (Q-Learning) veya RQL sistemleri ya da bunlarÄ±n hibrit/sofistike versiyonlarÄ±yla Ã§alÄ±ÅŸan bir bahis tahmin platformu geliÅŸtirmek.

## ğŸ—ï¸ Sistem Mimarisi

### 1. AltyapÄ± KararlarÄ±
- **Veri AkÄ±ÅŸÄ±:** Kafka ile streaming pipeline
- **Model Deployment:** Kubeflow + KServe InferenceService
- **Ã–lÃ§eklendirme:** HPA (Horizontal Pod Autoscaler) ile autoscaling
- **Monitoring:** Prometheus ile metrik toplama
- **Latency Hedefi:** KServe pod'larÄ±nda <10ms

### 2. Hierarchical Reinforcement Learning (HRL) YapÄ±sÄ±

#### Ãœst Katman (Manager Agent)
**GÃ¶rev:** Risk/bÃ¼tÃ§e yÃ¶netimi ve alt ajan seÃ§imi

**State VektÃ¶rÃ¼:**
```python
state = [
    bÃ¼tÃ§e_kalan,
    risk_score,
    portfÃ¶y_return,
    sub_agent_performance,
    market_volatility
]
```

**Eylem SeÃ§imi:** Upper Confidence Bound (UCB) algoritmasÄ±
```python
class ManagerAgent:
    def __init__(self):
        self.arms = [
            {'agent': 'prematch', 'q': 0, 'n': 0, 't': 0},
            {'agent': 'live', 'q': 0, 'n': 0, 't': 0},
            {'agent': 'risk', 'q': 0, 'n': 0, 't': 0}
        ]
        self.roi_history = deque(maxlen=10)
    
    def select_action(self, state):
        arm = max(self.arms, 
                 key=lambda x: x['q'] + 0.2*np.sqrt(np.log(sum(a['t'] for a in self.arms))/(x['n']+1)))
        return arm['agent']
    
    def update_arm(self, arm, reward):
        arm['n'] += 1
        arm['t'] += 1
        arm['q'] += (reward - arm['q']) / arm['n']
```

#### Alt Katmanlar (Worker Agents)

**MaÃ§ Ã–ncesi Agent:** Tabular Q-Learning
- Statik verileri iÅŸler
- MaÃ§ Ã¶ncesi bahis kararlarÄ±

**CanlÄ± Agent:** LSTM + PPO (Proximal Policy Optimization)
```python
class LiveAgent(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=live_feats, hidden_size=64)
        self.attention = nn.Linear(64, 1)
        self.actor = nn.Linear(64, action_space)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        attention_weights = F.softmax(self.attention(lstm_out), dim=1)
        context = torch.sum(attention_weights * lstm_out, dim=1)
        return self.actor(context)
```

**Risk Agent:** Upper Confidence Bound (UCB) algoritmasÄ±

### 3. Ã–dÃ¼l Fonksiyonu (Reward Function)

**Nihai FormÃ¼l:**
```python
def compute_reward(state, payout, stake):
    # ROI Hesaplama
    action_roi = (payout - stake) / (stake + 1e-6)
    
    # Risk AyarlÄ± Getiri (Sharpe OranÄ± YaklaÅŸÄ±mÄ±)
    risk_adjusted_return = action_roi / (state.market_volatility * state.risk_score + 1e-6)
    
    # BÃ¼tÃ§e KÄ±sÄ±tÄ± CezasÄ±
    budget_penalty = 0.1 * max(0, 0.8 - state.bÃ¼tÃ§e_kalan / state.initial_budget)
    
    # Dinamik BaÅŸabaÅŸ NoktasÄ± ile Performans Bonusu
    break_even = 1.0 / (state.avg_odds + 1e-6)
    performance_bonus = 0.2 * (state.sub_agent_performance - break_even)
    
    return risk_adjusted_return - budget_penalty + performance_bonus
```

**Ã–dÃ¼l Fonksiyonu BileÅŸenleri:**
1. **Risk AyarlÄ± Getiri:** Sharpe oranÄ± prensibi ile volatilite ve risk skorunu hesaba katar
2. **BÃ¼tÃ§e CezasÄ±:** BÃ¼tÃ§e %80'in altÄ±na dÃ¼ÅŸtÃ¼ÄŸÃ¼nde ceza uygular
3. **Performans Bonusu:** Alt ajan performansÄ±nÄ± dinamik baÅŸabaÅŸ noktasÄ±na gÃ¶re deÄŸerlendirir

### 4. Performans Takibi

**ROI GeÃ§miÅŸi YÃ¶netimi:**
```python
from collections import deque

class ManagerAgent:
    def __init__(self):
        self.roi_history = deque(maxlen=10)  # O(1) karmaÅŸÄ±klÄ±k, sabit bellek
    
    def update_performance(self, latest_roi):
        self.roi_history.append(latest_roi)  # Otomatik eski veri silinir
        state.sub_agent_performance = np.mean(self.roi_history)
```

**Optimizasyon KararÄ±:**
- `pop(0)` yerine `deque(maxlen=10)` kullanÄ±mÄ±
- **Sebep:** O(1) karmaÅŸÄ±klÄ±k vs O(n), GC yÃ¼kÃ¼ azaltma
- **KazanÃ§:** %20 throughput artÄ±ÅŸÄ±

## ğŸ”‘ Kritik Teknik Kararlar

### 1. Ã‡atÄ±ÅŸma: Ã‡arpÄ±m vs Sharpe OranÄ±
- **DevOps Ã–nerisi:** `reward = portfÃ¶y_return * sub_agent_performance - 0.5 * market_volatility * risk_score`
- **Alfa Ä°tirazÄ±:** Ã‡arpÄ±m kararsÄ±zlÄ±k yaratÄ±r
- **Nihai Karar:** Sharpe OranÄ± yaklaÅŸÄ±mÄ± (risk ayarlÄ± getiri)

### 2. Ã‡atÄ±ÅŸma: Sabit vs Dinamik BaÅŸabaÅŸ
- **Gamma Ã–nerisi:** Sabit 0.5 (bahis pazarÄ±nda %50 baÅŸarÄ±)
- **Alfa Ã–nerisi:** Dinamik `break_even = 1 / avg_odds`
- **Nihai Karar:** Dinamik baÅŸabaÅŸ noktasÄ± (piyasa koÅŸullarÄ±na uyum)

### 3. Ã‡atÄ±ÅŸma: pop(0) vs Slicing vs Deque
- **Beta:** `pop(0)` ile liste yÃ¶netimi
- **Gamma:** Slicing ile son 10 eleman
- **DevOps/Alfa:** `deque(maxlen=10)` Ã¶nerisi
- **Nihai Karar:** Deque (O(1) karmaÅŸÄ±klÄ±k, sabit bellek, %20 performans artÄ±ÅŸÄ±)

## ğŸ“Š Sistem AkÄ±ÅŸÄ±

1. **Veri Toplama:** Kafka ile canlÄ± maÃ§ verileri ve bahis bÃ¼lteni akÄ±ÅŸÄ±
2. **State GÃ¼ncelleme:** Her 10 saniyede bir Kafka consumer ile
3. **Manager KararÄ±:** UCB ile en uygun alt ajan seÃ§imi
4. **Alt Ajan Aksiyonu:** SeÃ§ilen ajana gÃ¶re bahis kararÄ±
5. **Ã–dÃ¼l Hesaplama:** SonuÃ§ alÄ±ndÄ±ktan sonra reward fonksiyonu
6. **Model GÃ¼ncelleme:** UCB arm parametreleri ve performans metriklerinin gÃ¼ncellenmesi

## ğŸ“ Ã–ÄŸrenilen Dersler

1. **Volatilite YÃ¶netimi:** Ã‡arpÄ±msal formÃ¼ller yerine oranlama yaklaÅŸÄ±mlarÄ± daha stabil
2. **Dinamik Adaptasyon:** Sabit eÅŸik deÄŸerleri yerine piyasa koÅŸullarÄ±na gÃ¶re ayarlanan metrikler
3. **Performans Optimizasyonu:** Veri yapÄ±sÄ± seÃ§imi (deque) %20 throughput farkÄ± yaratabilir
4. **Bellek YÃ¶netimi:** GC yÃ¼kÃ¼nÃ¼ azaltmak yÃ¼ksek frekanslÄ± sistemlerde kritik

## ğŸš€ Deployment PlanÄ±

1. ROI geÃ§miÅŸi iÃ§in `deque(maxlen=10)` kullanÄ±mÄ±
2. `action_roi` tanÄ±mÄ±: `(payout - stake) / stake`
3. Nihai Ã¶dÃ¼l fonksiyonu: Sharpe yaklaÅŸÄ±mÄ± + dinamik baÅŸabaÅŸ + bÃ¼tÃ§e cezasÄ±
4. Kubeflow pipeline'a entegrasyon
5. Prometheus ile monitoring
6. KServe ile production deployment

